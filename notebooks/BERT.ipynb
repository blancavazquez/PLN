{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjUyRszafaImvGrKKy22GS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blancavazquez/PLN/blob/main/notebooks/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT\n",
        "\n",
        "El objetivo de esta libreta es usar el modelo BERT en diferentes tareas."
      ],
      "metadata": {
        "id": "plg0obLN_Yqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizando textos"
      ],
      "metadata": {
        "id": "4vTrGh2I_kld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Orb-VG9Rkj"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "text = 'UNAM: It was founded on September 21, 1551, under the name of the Royal and Pontifical University of Mexico. It is the largest and most important university in Mexico. Throughout time, it has played a leading role in the history and formation of our country. UNAM is recognized as a space of freedom. Respect, tolerance, and dialogue are practiced. The plurality of ideas and thought is appreciated as a sign of its wealth.'\n",
        " ## Tokenize and encode the text\n",
        "encoding = tokenizer.encode(text)\n",
        "print(\"Token IDs:\", encoding)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert token IDs back to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoding)\n",
        "print(f\"Original text: {text}\")\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "id": "nbcOzd94-TSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de sentimientos"
      ],
      "metadata": {
        "id": "CorL23WR-kqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        ")\n",
        "\n",
        "# Test text\n",
        "text = \"I absolutely love this product! Would buy again.\"\n",
        "\n",
        "# Get the sentiment\n",
        "result = sentiment_analyzer(text)\n",
        "print(f\"Sentiment: {result[0]['label']}\")\n",
        "print(f\"Confidence: {result[0]['score']:.4f}\")"
      ],
      "metadata": {
        "id": "4A7puStn-kVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de sentimientos (sin pipeline)"
      ],
      "metadata": {
        "id": "6ljmUnKP-2Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class BERTSentimentAnalyzer:\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.labels = ['NEGATIVE', 'POSITIVE']\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Tokenize with BERT-specific tokens\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move to GPU if available\n",
        "        return {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "    def predict(self, text):\n",
        "        # Prepare text for model\n",
        "        inputs = self.preprocess_text(text)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "        # Convert to human-readable format\n",
        "        prediction_dict = {\n",
        "            'text': text,\n",
        "            'sentiment': self.labels[probabilities.argmax().item()],\n",
        "            'confidence': probabilities.max().item(),\n",
        "            'probabilities': {\n",
        "                label: prob.item()\n",
        "                for label, prob in zip(self.labels, probabilities[0])\n",
        "            }\n",
        "        }\n",
        "        return prediction_dict"
      ],
      "metadata": {
        "id": "qR1nPsRN-1vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_sentiment_analysis():\n",
        "    # Initialize analyzer\n",
        "    analyzer = BERTSentimentAnalyzer()\n",
        "\n",
        "    # Test texts\n",
        "    texts = [\n",
        "        \"This product completely transformed my workflow!\",\n",
        "        \"Terrible experience, would not recommend.\",\n",
        "        \"It's decent for the price, but nothing special.\"\n",
        "    ]\n",
        "\n",
        "    # Analyze each text\n",
        "    for text in texts:\n",
        "        result = analyzer.predict(text)\n",
        "        print(f\"\\nText: {result['text']}\")\n",
        "        print(f\"Sentiment: {result['sentiment']}\")\n",
        "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "        print(\"Detailed probabilities:\")\n",
        "        for label, prob in result['probabilities'].items():\n",
        "            print(f\"  {label}: {prob:.4f}\")\n",
        "\n",
        "# Running demonstration\n",
        "demonstrate_sentiment_analysis()"
      ],
      "metadata": {
        "id": "b6K3Knmm_C3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocimiento de entidades (NER)"
      ],
      "metadata": {
        "id": "uvgWglvs_J03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "class BERTNamedEntityRecognizer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def recognize_entities(self, text):\n",
        "        # Tokenize input text\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # Move inputs to device\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        # print(inputs)\n",
        "\n",
        "        # Get predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            predictions = outputs.logits.argmax(-1)\n",
        "\n",
        "        # Convert predictions to entities\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "        labels = [self.model.config.id2label[p.item()] for p in predictions[0]]\n",
        "        # print(labels)\n",
        "\n",
        "        # Extract entities\n",
        "        entities = []\n",
        "        current_entity = None\n",
        "\n",
        "        for token, label in zip(tokens, labels):\n",
        "            if label.startswith('B-'):\n",
        "                if current_entity:\n",
        "                    entities.append(current_entity)\n",
        "                current_entity = {'type': label[2:], 'text': token}\n",
        "            elif label.startswith('I-') and current_entity:\n",
        "                if token.startswith('##'):\n",
        "                    current_entity['text'] += token[2:]\n",
        "                else:\n",
        "                    current_entity['text'] += ' ' + token\n",
        "            elif label == 'O':\n",
        "                if current_entity:\n",
        "                    entities.append(current_entity)\n",
        "                    current_entity = None\n",
        "\n",
        "        if current_entity:\n",
        "            entities.append(current_entity)\n",
        "\n",
        "        return entities"
      ],
      "metadata": {
        "id": "J0kO66Aa_MSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_ner():\n",
        "    # Initialize recognizer\n",
        "    ner = BERTNamedEntityRecognizer()\n",
        "\n",
        "    # Example text\n",
        "    text = \"\"\"\n",
        "    Apple CEO Tim Cook announced new AI features at their headquarters\n",
        "    in Cupertino, California. Microsoft and Google are also investing\n",
        "    heavily in artificial intelligence research.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get entities\n",
        "    entities = ner.recognize_entities(text)\n",
        "\n",
        "    # Display results\n",
        "    print(\"Found entities:\")\n",
        "    for entity in entities:\n",
        "        print(f\"- {entity['text']} ({entity['type']})\")\n",
        "\n",
        "# Running demonstration\n",
        "demonstrate_ner()"
      ],
      "metadata": {
        "id": "g8kja_21_QUV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}